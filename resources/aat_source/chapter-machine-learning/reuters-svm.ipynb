{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["nlp"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from __future__ import print_function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pprint\n", "import re\n", "try:\n", "    from html.parser import HTMLParser\n", "except ImportError:\n", "    from HTMLParser import HTMLParser"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.cross_validation import train_test_split\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.metrics import confusion_matrix\n", "from sklearn.svm import SVC"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ReutersParser(HTMLParser):\n", "    \"\"\"\n", "    ReutersParser subclasses HTMLParser and is used to open the SGML\n", "    files associated with the Reuters-21578 categorised test collection.\n", "    The parser is a generator and will yield a single document at a time.\n", "    Since the data will be chunked on parsing, it is necessary to keep \n", "    some internal state of when tags have been \"entered\" and \"exited\".\n", "    Hence the in_body, in_topics and in_topic_d boolean members.\n", "    \"\"\"\n", "    def __init__(self, encoding='latin-1'):\n", "        \"\"\"\n", "        Initialise the superclass (HTMLParser) and reset the parser.\n", "        Sets the encoding of the SGML files by default to latin-1.\n", "        \"\"\"\n", "        HTMLParser.__init__(self)\n", "        self._reset()\n", "        self.encoding = encoding\n", "    def _reset(self):\n", "        \"\"\"\n", "        This is called only on initialisation of the parser class\n", "        and when a new topic-body tuple has been generated. It\n", "        resets all off the state so that a new tuple can be subsequently\n", "        generated.\n", "        \"\"\"\n", "        self.in_body = False\n", "        self.in_topics = False\n", "        self.in_topic_d = False\n", "        self.body = \"\"\n", "        self.topics = []\n", "        self.topic_d = \"\"\n", "    def parse(self, fd):\n", "        \"\"\"\n", "        parse accepts a file descriptor and loads the data in chunks\n", "        in order to minimise memory usage. It then yields new documents\n", "        as they are parsed.\n", "        \"\"\"\n", "        self.docs = []\n", "        for chunk in fd:\n", "            self.feed(chunk.decode(self.encoding))\n", "            for doc in self.docs:\n", "                yield doc\n", "            self.docs = []\n", "        self.close()\n", "    def handle_starttag(self, tag, attrs):\n", "        \"\"\"\n", "        This method is used to determine what to do when the parser\n", "        comes across a particular tag of type \"tag\". In this instance\n", "        we simply set the internal state booleans to True if that particular\n", "        tag has been found.\n", "        \"\"\"\n", "        if tag == \"reuters\":\n", "            pass\n", "        elif tag == \"body\":\n", "            self.in_body = True\n", "        elif tag == \"topics\":\n", "            self.in_topics = True\n", "        elif tag == \"d\":\n", "            self.in_topic_d = True \n", "    def handle_endtag(self, tag):\n", "        \"\"\"\n", "        This method is used to determine what to do when the parser\n", "        finishes with a particular tag of type \"tag\". \n", "        If the tag is a <REUTERS> tag, then we remove all \n", "        white-space with a regular expression and then append the \n", "        topic-body tuple.\n", "        If the tag is a <BODY> or <TOPICS> tag then we simply set\n", "        the internal state to False for these booleans, respectively.\n", "        If the tag is a <D> tag (found within a <TOPICS> tag), then we\n", "        append the particular topic to the \"topics\" list and \n", "        finally reset it.\n", "        \"\"\"\n", "        if tag == \"reuters\":\n", "            self.body = re.sub(r'\\s+', r' ', self.body)\n", "            self.docs.append( (self.topics, self.body) )\n", "            self._reset()\n", "        elif tag == \"body\":\n", "            self.in_body = False\n", "        elif tag == \"topics\":\n", "            self.in_topics = False\n", "        elif tag == \"d\":\n", "            self.in_topic_d = False\n", "            self.topics.append(self.topic_d)\n", "            self.topic_d = \"\"  \n", "    def handle_data(self, data):\n", "        \"\"\"\n", "        The data is simply appended to the appropriate member state\n", "        for that particular tag, up until the end closing tag appears.\n", "        \"\"\"\n", "        if self.in_body:\n", "            self.body += data\n", "        elif self.in_topic_d:\n", "            self.topic_d += data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def obtain_topic_tags():\n", "    \"\"\"\n", "    Open the topic list file and import all of the topic names\n", "    taking care to strip the trailing \"\\n\" from each word.\n", "    \"\"\"\n", "    topics = open(\n", "        \"data/all-topics-strings.lc.txt\", \"r\"\n", "    ).readlines()\n", "    topics = [t.strip() for t in topics]\n", "    return topics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def filter_doc_list_through_topics(topics, docs):\n", "    \"\"\"\n", "    Reads all of the documents and creates a new list of two-tuples\n", "    that contain a single feature entry and the body text, instead of\n", "    a list of topics. It removes all geographic features and only \n", "    retains those documents which have at least one non-geographic\n", "    topic.\n", "    \"\"\"\n", "    ref_docs = []\n", "    for d in docs:\n", "        if d[0] == [] or d[0] == \"\":\n", "            continue\n", "        for t in d[0]:\n", "            if t in topics:\n", "                d_tup = (t, d[1])\n", "                ref_docs.append(d_tup)\n", "                break\n", "    return ref_docs"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_tfidf_training_data(docs):\n", "    \"\"\"\n", "    Creates a document corpus list (by stripping out the\n", "    class labels), then applies the TF-IDF transform to this\n", "    list. \n", "    The function returns both the class label vector (y) and \n", "    the corpus token/feature matrix (X).\n", "    \"\"\"\n", "    # Create the training data class labels\n", "    y = [d[0] for d in docs]\n", "    \n", "    # Create the document corpus list\n", "    corpus = [d[1] for d in docs]\n\n", "    # Create the TF-IDF vectoriser and transform the corpus\n", "    vectorizer = TfidfVectorizer(min_df=1)\n", "    X = vectorizer.fit_transform(corpus)\n", "    return X, y"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_svm(X, y):\n", "    \"\"\"\n", "    Create and train the Support Vector Machine.\n", "    \"\"\"\n", "    svm = SVC(C=1000000.0, gamma=\"auto\", kernel='rbf')\n", "    svm.fit(X, y)\n", "    return svm"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    # Create the list of Reuters data and create the parser\n", "    files = [\"data/reut2-%03d.sgm\" % r for r in range(0, 22)]\n", "    parser = ReutersParser()\n\n", "    # Parse the document and force all generated docs into\n", "    # a list so that it can be printed out to the console\n", "    docs = []\n", "    for fn in files:\n", "        for d in parser.parse(open(fn, 'rb')):\n", "            docs.append(d)\n\n", "    # Obtain the topic tags and filter docs through it \n", "    topics = obtain_topic_tags()\n", "    ref_docs = filter_doc_list_through_topics(topics, docs)\n", "    \n", "    # Vectorise and TF-IDF transform the corpus \n", "    X, y = create_tfidf_training_data(ref_docs)\n\n", "    # Create the training-test split of the data\n", "    X_train, X_test, y_train, y_test = train_test_split(\n", "        X, y, test_size=0.2, random_state=42\n", "    )\n\n", "    # Create and train the Support Vector Machine\n", "    svm = train_svm(X_train, y_train)\n\n", "    # Make an array of predictions on the test set\n", "    pred = svm.predict(X_test)\n\n", "    # Output the hit-rate and the confusion matrix for each model\n", "    print(svm.score(X_test, y_test))\n", "    print(confusion_matrix(pred, y_test))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}